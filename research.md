---
layout: page
title: My Research
permalink: /research/
---

### Dark matter mapping from gravitational lensing using gaussian and sparsity priors
[Link to paper](https://arxiv.org/abs/1801.08945)

[Link to UCL press release](https://www.ucl.ac.uk/mathematical-physical-sciences/news/2018/may/ucl-led-team-uses-new-data-science-techniques-dark-matter-maps)

This work uses 1.6 million galaxy images from the Dark Energy Survey (DES) Science Verification dataset that had previously been used, but now incorporates prior knowledge from theoretical models to improve the quality of the Dark Matter maps.


### Using Dataflow Engines to make a fast image MCMC sampler 
[Link to paper](https://arxiv.org/abs/1810.02821)

Dataflow computing is a new and powerful approach using reconfigurable hardware, which can be deeply pipelined and is intrinsically parallel. In this work we use Dataflow Engines (DFEs) for sampling realisations of noise-free images from the (Bayesian) Wiener posterior distribution given noisy and incomplete data. This work has particular applicability to cosmology, but the methods can be used for many data-intensive problems.


### Inferring parameters from data using noisy simulations
[Link to paper](https://arxiv.org/abs/1809.08246)

If we wanted to learn about cosmological parameters from data that cannot be modelled closed-form (writing out the equations explicity) we can still often run simulations of mock data to make predictions, which are compared with the observed data.

The statistical properties of the mock data have to be included in the parameter inference (in the likelihood distribution). Often this is ignored and excessive numbers of simulations are run to minimize any variance in the predictions. If instead the corrected likelihood is used, fewer simulations have to be run.


